{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69563190-c748-4807-81e7-89a582952397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json\n",
    "from glob import glob \n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random \n",
    "from typing import Any, Dict, List, Union\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login\n",
    "from dataclasses import dataclass\n",
    "from datasets import Dataset, load_dataset, DatasetDict, Audio\n",
    "\n",
    "\n",
    "from peft import PeftModel, prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# import evaluate\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    BitsAndBytesConfig, \n",
    "    AutoProcessor, \n",
    "    pipeline\n",
    ")\n",
    "\n",
    "import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2c80a7-d347-47a3-9012-ea10543ba8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"daje/korean-address-voice-v2\")\n",
    "\n",
    "# 데이터셋 로드 시 자동으로 16kHz로 리샘플링\n",
    "datasets = datasets.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309b0500-404a-4e24-8483-3af8c0fe5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/whisper-large-v3-turbo\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=\"Korean\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"Korean\", task=\"transcribe\")\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "base_model = base_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a9c272-8d66-4981-8bff-2f2901618e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 서울특별시 서초구 테헤란로 941, 현대아파트 553동 3722호.\n"
     ]
    }
   ],
   "source": [
    "# 오디오 가져오기\n",
    "audio = datasets[\"test\"][0][\"audio\"]\n",
    "\n",
    "# 전처리\n",
    "input_features = processor(\n",
    "    audio[\"array\"], \n",
    "    sampling_rate=audio[\"sampling_rate\"], \n",
    "    return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "input_features = input_features.to(\"cuda\")\n",
    "\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "    predicted_ids = base_model.generate(input_features)\n",
    "\n",
    "# 디코딩\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65ce98f-6c2c-4c60-80f9-2b4d7cf16836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b45159806f43ef89420980c9a34696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = [] \n",
    "no_train_result = [] \n",
    "for idx in tqdm(range(len(datasets[\"test\"]))):\n",
    "    # 오디오 가져오기\n",
    "    audio = datasets[\"test\"][idx][\"audio\"]\n",
    "    \n",
    "    # 전처리\n",
    "    input_features = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"], \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    input_features = input_features.to(\"cuda\")\n",
    "    \n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = base_model.generate(input_features)\n",
    "    \n",
    "    # 디코딩\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    no_train_result.append(transcription)\n",
    "    answers.append(datasets[\"test\"][idx][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c91a90-6a00-4f78-88c7-7f85caa40053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_address(text):\n",
    "    \"\"\"\n",
    "    주소 텍스트를 정규화하여 비교 가능하게 만듭니다.\n",
    "    \"\"\"\n",
    "    # 1. 소문자 변환 (영문이 섞인 경우 대비)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. 구두점 제거 (쉼표, 마침표, 하이픈 등)\n",
    "    text = re.sub(r'[,.\\-]', '', text)\n",
    "    \n",
    "    # 3. 모든 공백 제거\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 정규화 예시\n",
    "# 정규화전 : 광주광역시 북구 첨단과기로 208, 첨단하이파크 605동 1902호.\n",
    "# 정규화후: 광주광역시북구첨단과기로208 첨단아이파크605동1902호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0f78a3-ce25-49d1-af71-0317dcd9d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LoRA 어댑터 로드\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"/workspace/model-v2/checkpoint-60\"\n",
    ")\n",
    "\n",
    "# 4. GPU로 이동\n",
    "finetuned_model = finetuned_model.to(\"cuda\")\n",
    "\n",
    "# 5. 한국어 설정\n",
    "finetuned_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"ko\", \n",
    "    task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014b85f5-03ff-4b1a-b26c-2c6aba75aa2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f24ce076c634104978b706e7e6daeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = [] \n",
    "for idx in tqdm(range(len(datasets[\"test\"]))):\n",
    "    # 오디오 가져오기\n",
    "    audio = datasets[\"test\"][idx][\"audio\"]\n",
    "    \n",
    "    # 전처리\n",
    "    input_features = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"], \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    input_features = input_features.to(\"cuda\")\n",
    "    \n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = finetuned_model.generate(input_features)\n",
    "    \n",
    "    # 디코딩\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    train_result.append(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbb857f-3ddb-426e-8e3a-0f2d2b8fa799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Fine-tuning 전 - 틀린 예측 (상위 10개)\n",
      "================================================================================\n",
      "\n",
      "[1]\n",
      "정답: 서울특별시은평구마포대로571 래미안198동392호\n",
      "예측:  서울특별시 은평구 마포대로 572, 레미안 198동 392호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2]\n",
      "정답: 서울특별시노원구사당로302 엘에이치669동1290호\n",
      "예측:  서울특별시 노원구 사당로 302, LH 669, 동 1290호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3]\n",
      "정답: 서울특별시서초구선릉로337 파크자이1087동550호\n",
      "예측:  서울특별시 서초구 설릉로 337, 파크자이 1087동 550호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4]\n",
      "정답: 서울특별시노원구백제고분로755 호반베르디움983동4235호\n",
      "예측:  서울특별시 노원구 백제고분노 755, 호반베르디움 983동 4,235호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5]\n",
      "정답: 서울특별시관악구강남대로150 대우아파트929동1637호\n",
      "예측:  서울특별시 관악구 강남대로 150, 대호아파트 929동 1637호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6]\n",
      "정답: 서울특별시도봉구선릉로850 롯데캐슬498동1702호\n",
      "예측:  서울특별시 도봉구 설릉로 850, 롯데캐슬 498, 동1702호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[7]\n",
      "정답: 서울특별시서대문구여의대로576 엘에이치727동3368호\n",
      "예측:  서울특별시 서대문구 여의대로 576, LH 727, 동 3368호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[8]\n",
      "정답: 서울특별시도봉구천호대로775 e편한세상504동2107호\n",
      "예측:  서울특별시 도봉구 천호대로 775, 이 편안세상 504동 2107호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[9]\n",
      "정답: 서울특별시강북구고덕로481 래미안775동882호\n",
      "예측:  서울특별시 강북구 고덕로 482, 레미안 775동 882호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[10]\n",
      "정답: 서울특별시관악구고덕로424 래미안936동169호\n",
      "예측:  서울특별시 관악구 고덕로 424, 레미안 936동 169호.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "총 340개 중 176개 틀림\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Fine-tuning 후 - 틀린 예측 (상위 10개)\n",
      "================================================================================\n",
      "\n",
      "[1]\n",
      "정답: 강원특별자치도정선군중앙로318 GS자이1074동4073호\n",
      "예측: 강원특별자치도정성군중앙로318 GS자이1074동4073호\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2]\n",
      "정답: 충청남도천안시서북구배방로390 한화꿈에그린1066동1698호\n",
      "예측: 충청남도천안시서북구대방로390 한화꿈에그린1066동1698호\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3]\n",
      "정답: 전북특별자치도군산시익산대로506 아이파크663동4331호\n",
      "예측: 전북특별자치도군산시입산대로506 아이파크663동4331호\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "340개 중 총 3개 틀림\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning 전 - 틀린 케이스만 출력\n",
    "print(\"=\"*80)\n",
    "print(\"Fine-tuning 전 - 틀린 예측 (상위 10개)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "count = 0\n",
    "for idx in range(len(no_train_result)):\n",
    "    pred = normalize_address(no_train_result[idx])\n",
    "    answer = normalize_address(answers[idx])\n",
    "    \n",
    "    if pred != answer:\n",
    "        count += 1\n",
    "        print(f\"\\n[{count}]\")\n",
    "        print(f\"정답: {answers[idx]}\")\n",
    "        print(f\"예측: {no_train_result[idx]}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        if count >= 10:\n",
    "            break\n",
    "\n",
    "print(f\"\\n총 340개 중 {sum(1 for i in range(len(no_train_result)) if normalize_address(no_train_result[i]) != normalize_address(answers[i]))}개 틀림\")\n",
    "\n",
    "\n",
    "# Fine-tuning 후 - 틀린 케이스만 출력\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"Fine-tuning 후 - 틀린 예측 (상위 10개)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "count = 0\n",
    "for idx in range(len(train_result)):\n",
    "    pred = normalize_address(train_result[idx])\n",
    "    answer = normalize_address(answers[idx])\n",
    "    \n",
    "    if pred != answer:\n",
    "        count += 1\n",
    "        print(f\"\\n[{count}]\")\n",
    "        print(f\"정답: {answers[idx]}\")\n",
    "        print(f\"예측: {train_result[idx]}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        if count >= 10:\n",
    "            break\n",
    "\n",
    "print(f\"\\n340개 중 총 {sum(1 for i in range(len(train_result)) if normalize_address(train_result[i]) != normalize_address(answers[i]))}개 틀림\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d498a15-9ab4-45b6-bff9-ed57f39b8f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning 전 CER: 3.32%\n",
      "Fine-tuning 후 CER: 0.03%\n"
     ]
    }
   ],
   "source": [
    "cer_metric = evaluate.load(\"cer\")\n",
    "def calculate_cer(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Character Error Rate 계산 (Hugging Face evaluate 라이브러리 사용)\n",
    "    \"\"\"\n",
    "    # 정규화된 텍스트로 CER 계산\n",
    "    pred_normalized = [normalize_address(pred) for pred in predictions]\n",
    "    gt_normalized = [normalize_address(gt) for gt in ground_truths]\n",
    "    \n",
    "    # CER 계산 (0~1 범위로 반환되므로 100을 곱해 퍼센트로 변환)\n",
    "    cer = 100 * cer_metric.compute(predictions=pred_normalized, references=gt_normalized)\n",
    "    \n",
    "    return cer\n",
    "\n",
    "cer_no_train = calculate_cer(no_train_result, answers)\n",
    "cer_train = calculate_cer(train_result, answers)\n",
    "print(f\"Fine-tuning 전 CER: {cer_no_train:.2f}%\")\n",
    "print(f\"Fine-tuning 후 CER: {cer_train:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f28bb4c-9265-47b3-afb9-23af00dd4330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning 전 WER: 361.32%\n",
      "Fine-tuning 후 WER: 0.44%\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def calculate_wer(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Word Error Rate 계산 (Hugging Face evaluate 라이브러리 사용)\n",
    "    \"\"\"\n",
    "    # 정규화된 텍스트로 WER 계산\n",
    "    pred_normalized = [pred for pred in predictions]\n",
    "    gt_normalized = [gt for gt in ground_truths]\n",
    "    \n",
    "    # WER 계산 (0~1 범위로 반환되므로 100을 곱해 퍼센트로 변환)\n",
    "    wer = 100 * metric.compute(predictions=pred_normalized, references=gt_normalized)\n",
    "    \n",
    "    return wer\n",
    "\n",
    "\n",
    "# 사용법:\n",
    "wer_no_train = calculate_wer(no_train_result, answers)\n",
    "wer_train = calculate_wer(train_result, answers)\n",
    "print(f\"Fine-tuning 전 WER: {wer_no_train:.2f}%\")\n",
    "print(f\"Fine-tuning 후 WER: {wer_train:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cc64977-30b1-49d8-8b95-12acaf34e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1035c1f7b244270a60dcef6fd996c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735df15f796e4387a3196741c4965717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/daje/whisper-v3-turbo-address/commit/b40320102e29039ebef62f3340df2264b088e959', commit_message='Upload processor', commit_description='', oid='b40320102e29039ebef62f3340df2264b088e959', pr_url=None, repo_url=RepoUrl('https://huggingface.co/daje/whisper-v3-turbo-address', endpoint='https://huggingface.co', repo_type='model', repo_id='daje/whisper-v3-turbo-address'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습한 모델 허깅페이스에 업로드하기\n",
    "merged_model = finetuned_model.merge_and_unload()\n",
    "merged_model.push_to_hub(\"daje/whisper-v3-turbo-address\")\n",
    "processor.push_to_hub(\"daje/whisper-v3-turbo-address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bd715-9d81-444a-9583-0b0e132a6b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
