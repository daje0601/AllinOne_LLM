{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6412a9-79a9-45ac-9253-a080f58a1018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480b8a8dcf3446729eaeb59cb4b84c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8028f04d86274a098072302893818833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8915924549c403fbd2584780011ebbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9740af2d5924bdd8146e1f4b3983304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aec645b356f4a89b2e3184436829e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585c5bc8e5bc4681bc7b6baad52c860f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9d276a0a5e4ebca3056debebb0ee4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/956M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "lora_adapter1 = \"daje/chapter5_psychological_chatbots\"\n",
    "lora_adapter1_path = snapshot_download(repo_id=lora_adapter1)\n",
    "lora_adapter2 = \"daje/chapter5_code-llama3-8B-text-to-sql-ver0.1\"\n",
    "lora_adapter2_path = snapshot_download(repo_id=lora_adapter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95af36d-c2ce-49ac-9349-58900af544aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-14 13:27:16 api_server.py:495] vLLM API server version 0.6.1.post2\n",
      "INFO 09-14 13:27:16 api_server.py:496] args: Namespace(model_tag='allganize/Llama-3-Alpha-Ko-8B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='lora_adapter1', path='/root/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e'), LoRAModulePath(name='lora_adapter2', path='/root/.cache/huggingface/hub/models--daje--chapter5_code-llama3-8B-text-to-sql-ver0.1/snapshots/dfa935ef610806c0b0207238a638dcbd50a68e9a')], prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, model='allganize/Llama-3-Alpha-Ko-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=True, max_loras=1, max_lora_rank=256, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x73b6cc097b00>)\n",
      "INFO 09-14 13:27:16 api_server.py:162] Multiprocessing frontend to use ipc:///tmp/4ea2d095-42a8-4b76-b3bf-f1f87c0750dd for RPC Path.\n",
      "INFO 09-14 13:27:16 api_server.py:178] Started engine process with PID 3060\n",
      "INFO 09-14 13:27:21 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='allganize/Llama-3-Alpha-Ko-8B-Instruct', speculative_config=None, tokenizer='allganize/Llama-3-Alpha-Ko-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allganize/Llama-3-Alpha-Ko-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 09-14 13:27:22 model_runner.py:997] Starting to load model allganize/Llama-3-Alpha-Ko-8B-Instruct...\n",
      "INFO 09-14 13:27:23 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.76it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.65it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]\n",
      "\n",
      "INFO 09-14 13:27:26 model_runner.py:1008] Loading model weights took 14.9634 GB\n",
      "INFO 09-14 13:27:46 gpu_executor.py:122] # GPU blocks: 27190, # CPU blocks: 2048\n",
      "INFO 09-14 13:27:51 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-14 13:27:51 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-14 13:28:09 model_runner.py:1430] Graph capturing finished in 18 secs.\n",
      "INFO 09-14 13:28:09 api_server.py:226] vLLM to use /tmp/tmp6setzhrp as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 09-14 13:28:09 serving_embedding.py:190] embedding_mode is False. Embedding API will not work.\n",
      "INFO 09-14 13:28:09 launcher.py:20] Available routes are:\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /docs, Methods: HEAD, GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /health, Methods: GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /tokenize, Methods: POST\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /detokenize, Methods: POST\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /v1/models, Methods: GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /version, Methods: GET\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /v1/completions, Methods: POST\n",
      "INFO 09-14 13:28:09 launcher.py:28] Route: /v1/embeddings, Methods: POST\n",
      "INFO 09-14 13:28:09 launcher.py:33] Launching Uvicorn with --limit_concurrency 32765. To avoid this limit at the expense of performance run with --disable-frontend-multiprocessing\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2988\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 09-14 13:28:19 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 09-14 13:28:29 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 09-14 13:28:31 logger.py:36] Received request cmpl-ac559ec8486f48febaeccfb8c09d8b2c-0: prompt: '오늘 너무 힘들어요!', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 58368, 105622, 109748, 110671, 65950, 105807, 0], lora_request: LoRARequest(lora_name='lora_adapter1', lora_int_id=1, lora_path='/root/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e', lora_local_path=None, long_lora_max_len=None), prompt_adapter_request: None.\n",
      "INFO 09-14 13:28:31 async_llm_engine.py:201] Added request cmpl-ac559ec8486f48febaeccfb8c09d8b2c-0.\n",
      "INFO 09-14 13:28:34 metrics.py:351] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 09-14 13:28:35 async_llm_engine.py:169] Finished request cmpl-ac559ec8486f48febaeccfb8c09d8b2c-0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:44888 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-14 13:28:35 logger.py:36] Received request chat-69ee002b48a54b9bb72c8261b32de3a5: prompt: '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n오늘 너무 힘든 하루였어요 ㅠㅠ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['<|eot_id|>', 'Human:', 'Assistant:'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=500, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 128006, 882, 128007, 271, 58368, 105622, 109748, 110671, 82776, 123106, 101574, 105807, 103667, 254, 111332, 128009, 128006, 78191, 128007, 271], lora_request: LoRARequest(lora_name='lora_adapter1', lora_int_id=1, lora_path='/root/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e', lora_local_path=None, long_lora_max_len=None), prompt_adapter_request: None.\n",
      "INFO 09-14 13:28:35 async_llm_engine.py:201] Added request chat-69ee002b48a54b9bb72c8261b32de3a5.\n",
      "INFO 09-14 13:28:35 async_llm_engine.py:169] Finished request chat-69ee002b48a54b9bb72c8261b32de3a5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:44888 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-14 13:28:35 logger.py:36] Received request cmpl-8bc2413e133146fa865c19a9cc32c5c7-0: prompt: 'Task:최고 총액을 말해줘.\\'\\nSQL table: CREATE TABLE table_12014 (\\n    \"Rider\" text,\\n    \"Horse\" text,\\n    \"Faults\" text,\\n    \"Round 1 + 2A Points\" text,\\n    \"Total\" real\\n)\\nSQL query:', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 6396, 25, 122695, 107152, 106446, 18359, 101264, 34983, 59269, 246, 24314, 6827, 2007, 25, 31876, 14700, 2007, 62, 4364, 975, 2456, 262, 330, 49, 1814, 1, 1495, 345, 262, 330, 39, 11073, 1, 1495, 345, 262, 330, 59880, 82, 1, 1495, 345, 262, 330, 28597, 220, 16, 489, 220, 17, 32, 21387, 1, 1495, 345, 262, 330, 7749, 1, 1972, 198, 340, 6827, 3319, 25], lora_request: LoRARequest(lora_name='lora_adapter2', lora_int_id=2, lora_path='/root/.cache/huggingface/hub/models--daje--chapter5_code-llama3-8B-text-to-sql-ver0.1/snapshots/dfa935ef610806c0b0207238a638dcbd50a68e9a', lora_local_path=None, long_lora_max_len=None), prompt_adapter_request: None.\n",
      "INFO 09-14 13:28:35 async_llm_engine.py:201] Added request cmpl-8bc2413e133146fa865c19a9cc32c5c7-0.\n",
      "INFO 09-14 13:28:37 async_llm_engine.py:169] Finished request cmpl-8bc2413e133146fa865c19a9cc32c5c7-0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:44888 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-14 13:28:37 logger.py:36] Received request chat-406cd660828f44019ba227917c0316be: prompt: '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nTask: tf103 차대의 총 포인트 수를 알려주세요.\\nSQL table: CREATE TABLE table_11482 (\\n    \"Year\" real,\\n    \"Chassis\" text,\\n    \"Engine\" text,\\n    \"Tyres\" text,\\n    \"Points\" real\\n)\\nSQL query:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[';\\n', '-- '], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=500, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 128006, 882, 128007, 271, 6396, 25, 6543, 6889, 103213, 123094, 107152, 99969, 121327, 29833, 18918, 116023, 92769, 627, 6827, 2007, 25, 31876, 14700, 2007, 62, 8011, 6086, 2456, 262, 330, 9679, 1, 1972, 345, 262, 330, 1163, 33567, 1, 1495, 345, 262, 330, 4674, 1, 1495, 345, 262, 330, 32974, 417, 1, 1495, 345, 262, 330, 11665, 1, 1972, 198, 340, 6827, 3319, 25, 128009, 128006, 78191, 128007, 271], lora_request: LoRARequest(lora_name='lora_adapter2', lora_int_id=2, lora_path='/root/.cache/huggingface/hub/models--daje--chapter5_code-llama3-8B-text-to-sql-ver0.1/snapshots/dfa935ef610806c0b0207238a638dcbd50a68e9a', lora_local_path=None, long_lora_max_len=None), prompt_adapter_request: None.\n",
      "INFO 09-14 13:28:37 async_llm_engine.py:201] Added request chat-406cd660828f44019ba227917c0316be.\n",
      "INFO 09-14 13:28:37 async_llm_engine.py:169] Finished request chat-406cd660828f44019ba227917c0316be.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:44888 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!vllm serve allganize/Llama-3-Alpha-Ko-8B-Instruct \\\n",
    "    --enable-lora \\\n",
    "    --lora-modules \\\n",
    "    lora_adapter1={lora_adapter1_path} \\\n",
    "    lora_adapter2={lora_adapter2_path} \\\n",
    "    --max-lora-rank 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f19db-0fdf-4ce6-9326-a1c31774094e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
